=t=Rolling Your Own=t=

First, it should go without saying that it's generally better to use and build upon an existing library rather than trying to roll your own from scratch. For one thing, you can get a number of features "for free" that way with no work required on your part. For another, developers outside of your team and projects work on those libraries, and in the words of Eric S. Raymond, "Given enough eyes, all bugs are shallow."

Be that as it may, you're likely to pursue doing this if you've read this far, so let's proceed.

=1=Sending Requests=1=

In addition to wrappers for specific protocols, the streams extension also offers socket transports for dealing with data at a lower level. One of these socket transports is for TCP, or Transmission Control Protocol, which is a core internet protocol used to ensure realiable delivery of an ordered sequence of bytes. The socket transport facilitates sending a raw data stream, in this case a manually constructed HTTP request, to a server.

<code php>
<?php
$stream = stream_socket_client('tcp://localhost.example:80');
$request = "GET / HTTP/1.1\r\nHost: localhost.example\r\n\r\n";
fwrite($stream, $request);
echo stream_get_contents($stream);
fclose($stream);

/*
Example output:
HTTP/1.1 200 OK
Date: Wed, 21 Jan 2009 03:16:43 GMT
Server: Apache/2.2.9 (Ubuntu) PHP/5.2.6-2ubuntu4 with Suhosin-Patch
X-Powered-By: PHP/5.2.6-2ubuntu4
Vary: Accept-Encoding
Content-Length: 12
Connection: close
Content-Type: text/html

Hello world!
*/
?>
</code>

- The ''stream_socket_client'' function is used to establish a connection with the server, returning a connection handle resource assigned to ''$stream''.
- ''tcp://'' specifies the transport to use.
- ''localhost.example'' is the hostname of the server.
- '':80'' specifies the port on which to connect to the server, in this case 80 because it is the standard port for HTTP. The port to use depends on the configuration of the web server.
- ''$request'' contains the request to be sent to the server, where individual lines are separated with a CRLF sequence (see Chapter 2 "GET Requests") and the request ends with a double CRLF sequence (effectively a blank line) to indicate to the server that the end of the request has been reached. Note that the request //must// contain the ending sequence or the web server will simply hang waiting for the rest of the request.
- The ''fwrite'' function is used to transmit the request over the established connection represented by ''$stream''.
- The ''stream_get_contents'' function is used to read all available data from the connection, in this case the response to the request.
- The ''fclose'' function is used to explicitly terminate the connection.

Depending on the nature and requirements of the project, not all facets of a request may be known at one time. In this situation, it is desirable to encapsulate request metadata in a data structure such as an associative array or an object. From this, a central unit of logic can be used to read that metadata and construct a request in the form of a string based on it.

Manually constructing requests within a string as shown in the example above also doesn't have ideal readability. If exact requests are known ahead of time and do not vary, an alternative approach is storing them in a data source of some type, then retrieving them at runtime and sending them over the connection as they are. Whether it is possible to take this approach depends on the level of variance in requests going between the web scraping application and the target application.

If the need arises to manually build query strings or URL-encoded POST request bodies, the ''http_build_query'' function allows this to be done using associative arrays.

=1=Parsing Responses=1=

Once you've received a response, the next step is obtaining the data you need from it. Taking the response from the last example, let's examine what this might look like.

<code php>
<?php
// Split the headers and body into separate variables
list($headers, $body) = explode("\r\n\r\n", $response, 2);

// Remove the status line from the headers
list($status, $headers) = explode("\r\n", $headers, 2);

// Parse the headers segment into individual headers
preg_match_all(
    "/(?P<name>[^:]+): (?P<value>[^\r]+)(?:$|\r\n[^ \t]*)/U",
    $headers,
    $headers,
    PREG_SET_ORDER
);
?>
</code>

Logic to separate individual headers must account for the ability of header values to span multiple lines as per RFC 2616 Section 2.2. As such, ''preg_match_all'' is used here to separate individual headers. See the later chapter on PCRE for more information on regular expressions. If a situation necessitates parsing data contained in URLs and query strings, check out the ''parse_url'' and ''parse_str'' functions. As with the request, it is generally desirable to parse response data into a data structure for ease of reference.

=1=Transfer Encoding=1=

Before parsing the body, the headers should be checked for a few things. If a ''Transfer-Encoding'' header is present and has a value of ''chunked'', it means that the server is sending the response back in chunks rather than all at once. The advantage to this is that the server does not have to wait until the entire response is composed before starting to return it (in order to determine and include its length in the ''Content-Length'' header), which can increase overall server throughput.

When each chunk is sent, it is preceded by a hexadecimal number to indicate the size of the chunk followed by a CRLF sequence. The end of each chunk is also denoted by a CRLF sequence. The end of the body is denoted with a chunk size of 0, which is particularly important when using a persistent connection since the client must know where one response ends and the next begins.

The ''strstr'' function can be used to obtain characters in a string prior to a newline. To convert strings containing hexadecimal numbers to their decimal equivalents, see the ''hexdec'' function. An example of what these two might look like in action is included below. The example assumpes that a request body has been written to a string. 

<code php>
<?php
$unchunked = '';
do {
    if ($length = hexdec(strstr($body, "\r\n", true))) {
        $body = ltrim(strstr($body, "\r\n"));
        $unchunked .= substr($body, 0, $length);
        $body = substr($body, $length + 2);
    }
} while ($length > 0);
?>
</code>

See Section 3.6.1 and Appendix 19.4.6 of RFC 2616 for more information on chunked transfer encoding.

=1=Content Encoding=1=

If the zlib extension is loaded (which can be checked using the ''extension_loaded'' function), the client can optionally include an ''Accept-Encoding'' header with a value of ''gzip,deflate'' in its request. If the server supports content compression, it will include a ''Content-Encoding'' header in its response with a value indicating which of the two compression schemes it used on the response body before sending it.

The purpose of this is to reduce the amount of data being sent to reduce bandwidth consumption and increase throughput (assuming that compression and decompression takes less time than data transfer, which is generally the case). Upon receiving the response, the client must decompress the response using the original scheme used by the server to compress it.

<code php>
<?php
// If Content-Encoding is gzip...
$decoded = gzinflate(substr($body, 10));

// If Content-Encoding is deflate...
$decoded = gzuncompress($body);
?>
</code>

- Yes, the function names are correct. One would think that ''gzinflate'' would be used to decode a body encoded using the ''deflate'' encoding scheme. Apparently this is just an oddity in the naming scheme used by the zlib library.
- When the encoding scheme is ''gzip'', a GZIP header is included in the response. ''gzinflate'' does not respond well to this. Hence, the header (contained in the first 10 bytes of the body) is stripped before the body is passed to ''gzinflate''.

See RFC 2616 Section 3.5 for more information on content encoding. RFC 1951 covers specifics of the DEFLATE algorithm on which the ''deflate'' encoding scheme is based while RFC 1952 details the gzip file format on which the ''gzip'' encoding scheme is based.

=1=Timing=1=

Each time a web server receives a request, a separate line of execution in the form of a process or thread is created or reused to deal with that request. Left unchecked, this could potentially cause all resources on a server to be consumed by a large request load. As such, web servers generally restrict the number of requests they can handle concurrently. A request beyond this limit would be blocked until the server completed an existing request for which it had already allocated resources. Requests left unserved too long eventually time out.

Throttling is a term used to describe a client overloading a server with requests to the point where it consumes the available resource pool and thereby delays or prevents the processing of requests, potentially including requests the client itself sent last. Obviously, it's desirable to avoid this behavior for two reasons: 1) it can be construed as abuse and result in being banned from accessing the server; 2) it prevents the client from being consistently functional.

Most web browsers will establish a maximum of four concurrent connections per domain name when loading a given resource and its dependencies. As such, this is a good starting point for testing the load of the server hosting the target application. When possible, measure the response times of individual requests and compare that to the number of concurrent requests being sent to determine how much of a load the server can withstand.

Depending on the application, real-time interaction with the target application may not be necessary. If interaction with the target application and the data it handles will be limited to the userbase of your web scraping application, it may be possible to retrieve data as necessary, cache it locally, store modifications locally, and push them to the target application in bulk during hours of non-peak usage. To discern what these hours are, observe response time with respect to the time of day in which requests are made to locate the time periods during which response times are consistently highest.
